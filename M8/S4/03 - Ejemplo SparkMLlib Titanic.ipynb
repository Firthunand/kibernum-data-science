{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a79a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear la SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TitanicSparkMLlib\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd7f78",
   "metadata": {},
   "source": [
    "## 2. Cargar los datos del Titanic\n",
    "\n",
    "Asumimos que el archivo `titanic.csv` está en la **misma carpeta** que este notebook (`M8/S4`).\n",
    "\n",
    "Usaremos `spark.read.csv` con `header=True` e `inferSchema=True` para que Spark detecte automáticamente los tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Cargar el archivo titanic.csv\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data_path = \"titanic.csv\"  # ruta relativa al notebook\n",
    "\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(data_path)\n",
    ")\n",
    "\n",
    "df_raw.count(), df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Mostrar algunas filas de ejemplo\n",
    "df_raw.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da305b",
   "metadata": {},
   "source": [
    "## 3. Análisis exploratorio básico (EDA)\n",
    "\n",
    "Realizaremos un EDA sencillo para entender el dataset:\n",
    "\n",
    "- Distribución de la variable objetivo de clasificación: `Survived`.\n",
    "- Valores nulos por columna.\n",
    "- Estadísticos básicos de variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a63148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Distribución de la variable objetivo de clasificación (Survived)\n",
    "df_raw.groupBy(\"Survived\").count().orderBy(\"Survived\").show()\n",
    "\n",
    "# 3.2 Conteo de filas totales\n",
    "print(\"Total de filas:\", df_raw.count())\n",
    "\n",
    "# 3.3 Valores nulos por columna\n",
    "null_counts = df_raw.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c)).alias(c) for c in df_raw.columns])\n",
    "null_counts.show(truncate=False)\n",
    "\n",
    "# 3.4 Estadísticos descriptivos de algunas columnas numéricas\n",
    "num_cols = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
    "df_raw.select(num_cols).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c696189",
   "metadata": {},
   "source": [
    "## 4. Limpieza y preprocesamiento de datos\n",
    "\n",
    "Para construir modelos en MLlib necesitamos:\n",
    "\n",
    "- Seleccionar columnas relevantes.\n",
    "- Manejar valores nulos (por ejemplo, imputar medias en numéricas).\n",
    "- Convertir variables categóricas a numéricas mediante `StringIndexer` y `OneHotEncoder`.\n",
    "- Unir todas las características en un único vector `features` con `VectorAssembler`.\n",
    "\n",
    "En este ejemplo usaremos como variables de entrada:\n",
    "\n",
    "- Numéricas: `Pclass`, `Age`, `SibSp`, `Parch`, `Fare`.\n",
    "- Categóricas: `Sex`, `Embarked`.\n",
    "\n",
    "Para clasificación, la etiqueta será `Survived`. Para regresión, usaremos `Fare` como variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Seleccionar columnas de interés y manejar nulos de forma simple\n",
    "selected_cols = [\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "df = df_raw.select([c for c in selected_cols])\n",
    "\n",
    "# Imputamos Age y Fare con la media (simple para el ejemplo)\n",
    "age_mean = df.select(F.avg(F.col(\"Age\")).alias(\"mean\")).collect()[0][0]\n",
    "fare_mean = df.select(F.avg(F.col(\"Fare\")).alias(\"mean\")).collect()[0][0]\n",
    "\n",
    "df = df.fillna({\"Age\": age_mean, \"Fare\": fare_mean})\n",
    "\n",
    "# Para columnas categóricas, reemplazamos nulos con un valor fijo (\"Unknown\")\n",
    "df = df.fillna({\"Embarked\": \"Unknown\"})\n",
    "\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee149d",
   "metadata": {},
   "source": [
    "## 5. Transformadores de características (feature engineering)\n",
    "\n",
    "A continuación definimos los **transformadores** que convertirán nuestras columnas brutas en un vector numérico que los modelos puedan usar:\n",
    "\n",
    "1. `StringIndexer` para convertir `Sex` y `Embarked` en índices numéricos.\n",
    "2. `OneHotEncoder` para transformar esos índices en vectores *one-hot*.\n",
    "3. `VectorAssembler` para unir columnas numéricas y vectores one-hot en una sola columna `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Definir columnas numéricas y categóricas\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "label_col_cls = \"Survived\"\n",
    "numeric_cols = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "categorical_cols = [\"Sex\", \"Embarked\"]\n",
    "\n",
    "# Indexers para variables categóricas\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=f\"{c}_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# OneHotEncoder para las columnas indexadas\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in categorical_cols],\n",
    "    outputCols=[f\"{c}_ohe\" for c in categorical_cols]\n",
    ")\n",
    "\n",
    "# VectorAssembler para unir todo en un único vector de características\n",
    "assembler_inputs = numeric_cols + [f\"{c}_ohe\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=assembler_inputs,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Guardamos todos los transformadores de características en una lista para reutilizarlos\n",
    "feature_stages = indexers + [encoder, assembler]\n",
    "feature_stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee96c6",
   "metadata": {},
   "source": [
    "## 6. Modelo de **clasificación**: predecir `Survived`\n",
    "\n",
    "Usaremos `LogisticRegression` como modelo de clasificación binaria.\n",
    "\n",
    "Pasos principales:\n",
    "\n",
    "1. Dividir los datos en entrenamiento y prueba.\n",
    "2. Construir un `Pipeline` que incluya:\n",
    "   - `StringIndexer` para la etiqueta `Survived` (columna `label`).\n",
    "   - Los transformadores de características (`feature_stages`).\n",
    "   - El modelo `LogisticRegression`.\n",
    "3. Ajustar el modelo y evaluar su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b263e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Definir el pipeline de clasificación con LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Indexer para la etiqueta (Survived) -> label\n",
    "label_indexer_cls = StringIndexer(\n",
    "    inputCol=label_col_cls,\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Modelo de regresión logística\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=50,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "# Pipeline completo de clasificación\n",
    "pipeline_cls = Pipeline(\n",
    "    stages=[label_indexer_cls] + feature_stages + [lr]\n",
    ")\n",
    "\n",
    "# 6.2 Dividir en train y test\n",
    "train_df_cls, test_df_cls = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Filas train (cls):\", train_df_cls.count())\n",
    "print(\"Filas test (cls):\", test_df_cls.count())\n",
    "\n",
    "# 6.3 Entrenar el modelo de clasificación\n",
    "model_cls = pipeline_cls.fit(train_df_cls)\n",
    "\n",
    "# 6.4 Obtener predicciones sobre el conjunto de prueba\n",
    "predictions_cls = model_cls.transform(test_df_cls)\n",
    "predictions_cls.select(\"Survived\", \"probability\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a73dc",
   "metadata": {},
   "source": [
    "## 7. Evaluación del modelo de clasificación\n",
    "\n",
    "Usaremos varias métricas para evaluar el modelo de clasificación:\n",
    "\n",
    "- `BinaryClassificationEvaluator` con métrica AUC (área bajo la curva ROC).\n",
    "- `MulticlassClassificationEvaluator` para calcular *accuracy* y *F1-score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Métricas para clasificación\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluador binario (AUC)\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = evaluator_auc.evaluate(predictions_cls)\n",
    "print(f\"AUC (ROC): {auc:.4f}\")\n",
    "\n",
    "# Evaluador multiclase para accuracy\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator_acc.evaluate(predictions_cls)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# F1-score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1 = evaluator_f1.evaluate(predictions_cls)\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7667ad",
   "metadata": {},
   "source": [
    "## 8. Modelo de **regresión**: predecir `Fare`\n",
    "\n",
    "Ahora construiremos un modelo de **regresión** para predecir la tarifa `Fare` en función de las mismas variables explicativas.\n",
    "\n",
    "Para ello:\n",
    "\n",
    "1. Usaremos el mismo conjunto de transformadores de características (`feature_stages`).\n",
    "2. Definiremos la etiqueta de regresión a partir de la columna `Fare`.\n",
    "3. Entrenaremos un `RandomForestRegressor`.\n",
    "4. Evaluaremos el modelo con métricas como RMSE y R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57813ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Preparar datos para regresión (predicción de Fare)\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Creamos un nuevo DataFrame con la etiqueta de regresión llamada 'label'\n",
    "df_reg = df.withColumnRenamed(\"Fare\", \"label\")\n",
    "\n",
    "# 8.2 Definir el modelo de regresión\n",
    "rf_reg = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 8.3 Pipeline de regresión: solo transformadores de características + modelo\n",
    "pipeline_reg = Pipeline(\n",
    "    stages=feature_stages + [rf_reg]\n",
    ")\n",
    "\n",
    "# 8.4 Dividir en train y test para regresión\n",
    "train_df_reg, test_df_reg = df_reg.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Filas train (reg):\", train_df_reg.count())\n",
    "print(\"Filas test (reg):\", test_df_reg.count())\n",
    "\n",
    "# 8.5 Entrenar modelo de regresión\n",
    "model_reg = pipeline_reg.fit(train_df_reg)\n",
    "\n",
    "# 8.6 Predicciones en test para regresión\n",
    "predictions_reg = model_reg.transform(test_df_reg)\n",
    "predictions_reg.select(\"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# 8.7 Evaluar con RMSE y R2\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator_rmse.evaluate(predictions_reg)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2 = evaluator_r2.evaluate(predictions_reg)\n",
    "print(f\"R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5166813",
   "metadata": {},
   "source": [
    "## 9. Guardar y reutilizar modelos entrenados (opcional)\n",
    "\n",
    "En proyectos reales es habitual **guardar** los modelos entrenados para reutilizarlos sin tener que reentrenar cada vez.\n",
    "\n",
    "Spark MLlib permite guardar modelos y pipelines con el método `.write().save(path)` y luego cargarlos con `.load(path)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Guardar el modelo de clasificación (pipeline completo)\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "save_path_cls = Path(\"models/titanic_logreg_pipeline\")\n",
    "\n",
    "# Borramos el directorio si ya existe (para no tener errores al sobrescribir)\n",
    "if save_path_cls.exists():\n",
    "    shutil.rmtree(save_path_cls)\n",
    "\n",
    "model_cls.write().save(str(save_path_cls))\n",
    "print(f\"Modelo de clasificación guardado en: {save_path_cls}\")\n",
    "\n",
    "# 9.2 Ejemplo de cómo se cargaría de nuevo (comentado para no ejecutarlo por defecto)\n",
    "# from pyspark.ml.pipeline import PipelineModel\n",
    "# loaded_model_cls = PipelineModel.load(str(save_path_cls))\n",
    "# loaded_model_cls.transform(test_df_cls).select(\"Survived\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d8c8e",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y próximos pasos\n",
    "\n",
    "En este notebook hemos visto un flujo relativamente completo de trabajo con **Spark MLlib** usando el dataset Titanic:\n",
    "\n",
    "- Carga y EDA básico con DataFrames.\n",
    "- Limpieza e imputación sencilla de valores nulos.\n",
    "- Preparación de datos con `StringIndexer`, `OneHotEncoder`, `VectorAssembler`.\n",
    "- Construcción de *pipelines* reutilizables.\n",
    "- Entrenamiento y evaluación de un modelo de **clasificación** (LogisticRegression).\n",
    "- Entrenamiento y evaluación de un modelo de **regresión** (RandomForestRegressor).\n",
    "- Ejemplo de guardado de modelos entrenados.\n",
    "\n",
    "Como **ejercicios adicionales** podrías:\n",
    "\n",
    "- Probar otros algoritmos de clasificación (p.ej. `RandomForestClassifier`, `GBTClassifier`).\n",
    "- Probar otros algoritmos de regresión (p.ej. `LinearRegression`, `GBTRegressor`).\n",
    "- Añadir **validación cruzada** (`CrossValidator`) y **búsqueda de hiperparámetros** (`ParamGridBuilder`).\n",
    "- Incluir más variables (por ejemplo, procesar el campo `Name` o `Cabin`).\n",
    "- Implementar un flujo de *ML Ops* donde este modelo se despliegue como servicio.\n",
    "\n",
    "Este cuaderno puede servirte como **plantilla base** para futuros proyectos de Machine Learning con Spark."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
